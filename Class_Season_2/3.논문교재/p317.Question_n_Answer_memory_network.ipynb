{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ...train.txt 내용\n",
    "1 Mary moved to the bathroom.<br>------------story\n",
    "2 John went to the hallway.<br>--------------story\n",
    "3 Where is Mary? (tab)bathroom(tab)1---------Question---An"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " a='\\t I\\tlove you .....\\n'\n",
    " a.strip()->'I \\t love you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train observations: 10000 Test observations: 1000\n"
     ]
    }
   ],
   "source": [
    "def get_data(infile): #위의 글 한 라인씩 읽음\n",
    "    stories, questions, answers = [], [], []\n",
    "    story_text = []\n",
    "    fin = open(infile, \"r\") \n",
    "    for line in fin:\n",
    "        line = line.strip()                # strip() : 양쪽 끝에 있는 공백, \\t, \\n 제거\n",
    "        lno, text = line.split(\" \", 1)     # 맨 앞의 라인 번호 분리, 1을 안써주면 공백마다 분리시킴, \n",
    "                                           # lno=1, text는 Mary moved to the bathroom.로 배정\n",
    "        if \"\\t\" in text:                   # 세 번째 문장에는 \\t가 두개 있음.\n",
    "            question, answer, _ = text.split(\"\\t\")\n",
    "            stories.append(story_text)     # 처음 두 문장\n",
    "            questions.append(question)     # 세 번째 문장의 질문\n",
    "            answers.append(answer)         # 세 번째 문장의 답변\n",
    "            story_text = []\n",
    "        else:\n",
    "            story_text.append(text)        # 처음 두 문장이 하나의 리스트에 들어감\n",
    "    fin.close()\n",
    "    return stories, questions, answers\n",
    "\n",
    "# get the data\n",
    "data_train = get_data(\"qa1_single-supporting-fact_train.txt\")\n",
    "data_test = get_data(\"qa1_single-supporting-fact_test.txt\")\n",
    "\n",
    "print(\"Train observations:\",len(data_train[0]),\"Test observations:\", len(data_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 에피소딕 스토리, 질문(Q), 답변(A) 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스토리 =  ['Mary moved to the bathroom.', 'John went to the hallway.']\n",
      "질  문 =  Where is Mary? \n",
      "답  변 =  bathroom\n"
     ]
    }
   ],
   "source": [
    "print(\"스토리 = \", data_train[0][0])\n",
    "print(\"질  문 = \", data_train[1][0])\n",
    "print(\"답  변 = \", data_train[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Vocab dictionary from Train & Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 22\n",
      "{'to': 1, 'the': 2, '.': 3, 'where': 4, 'is': 5, '?': 6, 'went': 7, 'john': 8, 'sandra': 9, 'mary': 10, 'daniel': 11, 'bathroom': 12, 'office': 13, 'garden': 14, 'hallway': 15, 'kitchen': 16, 'bedroom': 17, 'journeyed': 18, 'travelled': 19, 'back': 20, 'moved': 21, 'PAD': 0}\n"
     ]
    }
   ],
   "source": [
    "dictnry = collections.Counter()\n",
    "for stories, questions, answers in [data_train, data_test]:#[0]:story,[1]:question,[2]:answer\n",
    "    for story in stories:\n",
    "        for sent in story:\n",
    "            for word in nltk.word_tokenize(sent):\n",
    "                dictnry[word.lower()] += 1#사용빈도 count 작업\n",
    "                \n",
    "    for question in questions:\n",
    "        for word in nltk.word_tokenize(question):\n",
    "            dictnry[word.lower()] += 1\n",
    "            \n",
    "    for answer in answers:\n",
    "        for word in nltk.word_tokenize(answer):\n",
    "            dictnry[word.lower()] += 1\n",
    "\n",
    "word2indx = {w:(i+1) for i,(w,_) in enumerate(dictnry.most_common())}#dictionary읽기, 가장 빈도가 높은 순으로 \n",
    "#           숫자 붙여\n",
    "                                                                #()안에는 개수 지정\n",
    "word2indx[\"PAD\"] = 0       #padding 인덱스 붙임\n",
    "indx2word = {v:k for k,v in word2indx.items()}\n",
    "\n",
    "vocab_size = len(word2indx)\n",
    "print(\"vocabulary size:\",len(word2indx))\n",
    "print(word2indx)\n",
    "\n",
    "#빈도 높은 순으로 정렬,22단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute max sequence length for each entity (제일 긴 문장기준-길이 맞춰주기-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story maximum length: 14 Question maximum length: 4\n"
     ]
    }
   ],
   "source": [
    "story_maxlen = 0\n",
    "question_maxlen = 0\n",
    "\n",
    "# 2 문장의 story를 합쳤을 때의 단어 개수 = 길이\n",
    "for stories, questions, answers in [data_train, data_test]:\n",
    "    for story in stories: # stories = [\"   \", \"  \"]-->story 두 개씩 문장에 들어있던 것\n",
    "        story_len = 0\n",
    "        for sent in story:#위의 story의 sentence 하나 부르기\n",
    "            swords = nltk.word_tokenize(sent)#swords=[\"John\",\"went\",\"to\"...]->단어 개수\n",
    "            story_len += len(swords)\n",
    "        if story_len > story_maxlen:\n",
    "            story_maxlen = story_len #story의 최대 길이 구하기위해 실행된 문장임.\n",
    "            \n",
    "            #위에 것 question 버전\n",
    "    for question in questions:\n",
    "        question_len = len(nltk.word_tokenize(question))\n",
    "        if question_len > question_maxlen:\n",
    "            question_maxlen = question_len\n",
    "            \n",
    "print (\"Story maximum length:\",story_maxlen,\"Question maximum length:\",question_maxlen)\n",
    "#표준화 시키기 위해 각각 최대 길이를 찾아냄----->모든 스토리-14로 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.core import Activation, Dense, Dropout, Permute\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import add, concatenate, dot\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting data into Vectorized form\n",
    "Xstrain[0] = [0,0,8,21,1,2,12,3,9,7,1,2,13,3]   - 14개<br>\n",
    "Xqtrain[0] = [4,5,8,6] - 4개<br>\n",
    "Ytrain[0] = [0,0,0,0,...0,1,0,0,0,]   - 22개의 one-hot  \n",
    "  \n",
    "ex) Story=[\"John went to the garden.\",  \n",
    "           \"Sandara went to the bathroom\"]--Xstrain   \n",
    "    Question=[\"$where is sandara?\"]---------Xqtrain  \n",
    "    Answer=[\"bathroom\"]---------------------Ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John travelled to the hallway.', 'Mary journeyed to the bathroom.']\n",
      "[[8, 19, 1, 2, 15, 3], [10, 18, 1, 2, 12, 3]]\n",
      "[8, 19, 1, 2, 15, 3, 10, 18, 1, 2, 12, 3]\n"
     ]
    }
   ],
   "source": [
    "# list of list 연습\n",
    "for story, question, answer in zip(stories, questions, answers):\n",
    "    xs = [[word2indx[w.lower()] for w in nltk.word_tokenize(s)] for s in story]\n",
    "    print(story)\n",
    "    print(xs)\n",
    "    xs = list(itertools.chain.from_iterable(xs))\n",
    "    print(xs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train story (10000, 14) Train question (10000, 4) Train answer (10000, 22)\n",
      "Test story (1000, 14) Test question (1000, 4) Test answer (1000, 22)\n"
     ]
    }
   ],
   "source": [
    "def data_vectorization(data, word2indx, story_maxlen, question_maxlen):\n",
    "    Xs, Xq, Y = [], [], []\n",
    "    stories, questions, answers = data\n",
    "    for story, question, answer in zip(stories, questions, answers):\n",
    "        xs = [[word2indx[w.lower()] for w in nltk.word_tokenize(s)] for s in story]\n",
    "        xs = list(itertools.chain.from_iterable(xs))   # 2개 스토리를 하나로 합친다\n",
    "        xq = [word2indx[w.lower()] for w in nltk.word_tokenize(question)]\n",
    "        Xs.append(xs)\n",
    "        Xq.append(xq)\n",
    "        Y.append(word2indx[answer.lower()])\n",
    "        \n",
    "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
    "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
    "           np_utils.to_categorical(Y, num_classes=len(word2indx))\n",
    "\n",
    "Xstrain, Xqtrain, Ytrain = data_vectorization(data_train, word2indx, story_maxlen, question_maxlen)\n",
    "Xstest, Xqtest, Ytest = data_vectorization(data_test, word2indx, story_maxlen, question_maxlen)\n",
    "\n",
    "print(\"Train story\",Xstrain.shape,\"Train question\", Xqtrain.shape,\"Train answer\", Ytrain.shape)\n",
    "print( \"Test story\",Xstest.shape, \"Test question\",Xqtest.shape, \"Test answer\",Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0, 10, 21,  1,  2, 12,  3,  8,  7,  1,  2, 15,  3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xstrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 14, 128)      2816        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 4, 128)       2816        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 14, 128)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4, 128)       0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 14, 4)        88          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 14, 4)        0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 14, 4)        0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 14, 4)        0           dot_1[0][0]                      \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 4, 14)        0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 142)       0           permute_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           52992       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 22)           1430        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 22)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 60,142\n",
      "Trainable params: 60,142\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "LATENT_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 40\n",
    "\n",
    "# Inputs\n",
    "story_input = Input(shape=(story_maxlen,))\n",
    "question_input = Input(shape=(question_maxlen,))\n",
    "\n",
    "# Story encoder embedding\n",
    "story_encoder = Embedding(input_dim=vocab_size, output_dim=EMBEDDING_SIZE,\n",
    "                         input_length=story_maxlen)(story_input)\n",
    "story_encoder = Dropout(rate = 0.2)(story_encoder)\n",
    "\n",
    "# Question encoder embedding\n",
    "question_encoder = Embedding(input_dim=vocab_size,output_dim=EMBEDDING_SIZE,\n",
    "                            input_length=question_maxlen)(question_input)\n",
    "question_encoder = Dropout(rate = 0.3)(question_encoder)\n",
    "\n",
    "# Match between story and question\n",
    "match = dot([story_encoder, question_encoder], axes=[2,2])\n",
    "\n",
    "# Encode story into vector space of question\n",
    "story_encoder_c = Embedding(input_dim=vocab_size,output_dim=question_maxlen,\n",
    "                           input_length=story_maxlen)(story_input)\n",
    "story_encoder_c = Dropout(rate = 0.3)(story_encoder_c)\n",
    "\n",
    "# Combine match and story vectors\n",
    "response = add([match, story_encoder_c])\n",
    "response = Permute((2, 1))(response)\n",
    "\n",
    "# Combine response and question vectors to answers space\n",
    "answer = concatenate([response, question_encoder], axis=-1)\n",
    "answer = LSTM(LATENT_SIZE)(answer)\n",
    "answer = Dropout(rate = 0.2)(answer)\n",
    "answer = Dense(vocab_size)(answer)\n",
    "output = Activation(\"softmax\")(answer)\n",
    "\n",
    "model = Model(inputs=[story_input, question_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/40\n",
      "10000/10000 [==============================] - 5s 504us/step - loss: 2.0352 - acc: 0.1728 - val_loss: 1.7256 - val_acc: 0.2910\n",
      "Epoch 2/40\n",
      "10000/10000 [==============================] - 1s 132us/step - loss: 1.6734 - acc: 0.2757 - val_loss: 1.6406 - val_acc: 0.3610\n",
      "Epoch 3/40\n",
      "10000/10000 [==============================] - 1s 150us/step - loss: 1.5564 - acc: 0.4137 - val_loss: 1.4667 - val_acc: 0.5130\n",
      "Epoch 4/40\n",
      "10000/10000 [==============================] - 1s 145us/step - loss: 1.4028 - acc: 0.5132 - val_loss: 1.3703 - val_acc: 0.5260\n",
      "Epoch 5/40\n",
      "10000/10000 [==============================] - 1s 134us/step - loss: 1.3364 - acc: 0.5206 - val_loss: 1.3201 - val_acc: 0.5270\n",
      "Epoch 6/40\n",
      "10000/10000 [==============================] - 2s 157us/step - loss: 1.2953 - acc: 0.5160 - val_loss: 1.2974 - val_acc: 0.5220\n",
      "Epoch 7/40\n",
      "10000/10000 [==============================] - 1s 137us/step - loss: 1.2659 - acc: 0.5239 - val_loss: 1.2791 - val_acc: 0.5200\n",
      "Epoch 8/40\n",
      "10000/10000 [==============================] - 2s 164us/step - loss: 1.2536 - acc: 0.5213 - val_loss: 1.2803 - val_acc: 0.5210\n",
      "Epoch 9/40\n",
      "10000/10000 [==============================] - 2s 156us/step - loss: 1.2351 - acc: 0.5313 - val_loss: 1.2542 - val_acc: 0.5580\n",
      "Epoch 10/40\n",
      "10000/10000 [==============================] - 1s 144us/step - loss: 1.1805 - acc: 0.5845 - val_loss: 1.1645 - val_acc: 0.6130\n",
      "Epoch 11/40\n",
      "10000/10000 [==============================] - 1s 142us/step - loss: 1.0779 - acc: 0.6509 - val_loss: 1.0154 - val_acc: 0.6870\n",
      "Epoch 12/40\n",
      "10000/10000 [==============================] - 1s 149us/step - loss: 0.9031 - acc: 0.7244 - val_loss: 0.8420 - val_acc: 0.7570\n",
      "Epoch 13/40\n",
      "10000/10000 [==============================] - 1s 145us/step - loss: 0.7687 - acc: 0.7687 - val_loss: 0.7371 - val_acc: 0.7540\n",
      "Epoch 14/40\n",
      "10000/10000 [==============================] - 2s 157us/step - loss: 0.6881 - acc: 0.7833 - val_loss: 0.6660 - val_acc: 0.7560\n",
      "Epoch 15/40\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 0.6342 - acc: 0.7867 - val_loss: 0.6187 - val_acc: 0.7580\n",
      "Epoch 16/40\n",
      "10000/10000 [==============================] - 2s 172us/step - loss: 0.5944 - acc: 0.7884 - val_loss: 0.5972 - val_acc: 0.7560\n",
      "Epoch 17/40\n",
      "10000/10000 [==============================] - 1s 146us/step - loss: 0.5675 - acc: 0.7953 - val_loss: 0.5864 - val_acc: 0.7520\n",
      "Epoch 18/40\n",
      "10000/10000 [==============================] - 1s 148us/step - loss: 0.5506 - acc: 0.7917 - val_loss: 0.5742 - val_acc: 0.7530\n",
      "Epoch 19/40\n",
      "10000/10000 [==============================] - 1s 147us/step - loss: 0.5321 - acc: 0.7969 - val_loss: 0.5684 - val_acc: 0.7590\n",
      "Epoch 20/40\n",
      "10000/10000 [==============================] - 1s 150us/step - loss: 0.5248 - acc: 0.7927 - val_loss: 0.5689 - val_acc: 0.7560\n",
      "Epoch 21/40\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 0.5109 - acc: 0.8020 - val_loss: 0.5702 - val_acc: 0.7570\n",
      "Epoch 22/40\n",
      "10000/10000 [==============================] - 2s 156us/step - loss: 0.5057 - acc: 0.7983 - val_loss: 0.5656 - val_acc: 0.7570\n",
      "Epoch 23/40\n",
      "10000/10000 [==============================] - 2s 157us/step - loss: 0.4967 - acc: 0.8015 - val_loss: 0.5694 - val_acc: 0.7480\n",
      "Epoch 24/40\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 0.4894 - acc: 0.8003 - val_loss: 0.5605 - val_acc: 0.7610\n",
      "Epoch 25/40\n",
      "10000/10000 [==============================] - 2s 150us/step - loss: 0.4824 - acc: 0.8046 - val_loss: 0.5557 - val_acc: 0.7650\n",
      "Epoch 26/40\n",
      "10000/10000 [==============================] - 2s 156us/step - loss: 0.4763 - acc: 0.8056 - val_loss: 0.5685 - val_acc: 0.7590\n",
      "Epoch 27/40\n",
      "10000/10000 [==============================] - 2s 153us/step - loss: 0.4686 - acc: 0.8079 - val_loss: 0.5569 - val_acc: 0.7570\n",
      "Epoch 28/40\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 0.4673 - acc: 0.8070 - val_loss: 0.5618 - val_acc: 0.7500\n",
      "Epoch 29/40\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 0.4650 - acc: 0.8117 - val_loss: 0.5656 - val_acc: 0.7560\n",
      "Epoch 30/40\n",
      "10000/10000 [==============================] - 2s 162us/step - loss: 0.4569 - acc: 0.8130 - val_loss: 0.5614 - val_acc: 0.7610\n",
      "Epoch 31/40\n",
      "10000/10000 [==============================] - 2s 183us/step - loss: 0.4537 - acc: 0.8175 - val_loss: 0.5684 - val_acc: 0.7550\n",
      "Epoch 32/40\n",
      "10000/10000 [==============================] - 2s 168us/step - loss: 0.4447 - acc: 0.8205 - val_loss: 0.5720 - val_acc: 0.7580\n",
      "Epoch 33/40\n",
      "10000/10000 [==============================] - 2s 170us/step - loss: 0.4438 - acc: 0.8167 - val_loss: 0.5670 - val_acc: 0.7570\n",
      "Epoch 34/40\n",
      "10000/10000 [==============================] - 2s 172us/step - loss: 0.4440 - acc: 0.8226 - val_loss: 0.5745 - val_acc: 0.7560\n",
      "Epoch 35/40\n",
      "10000/10000 [==============================] - 2s 164us/step - loss: 0.4395 - acc: 0.8234 - val_loss: 0.5758 - val_acc: 0.7540\n",
      "Epoch 36/40\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 0.4359 - acc: 0.8235 - val_loss: 0.5708 - val_acc: 0.7560\n",
      "Epoch 37/40\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 0.4321 - acc: 0.8282 - val_loss: 0.5865 - val_acc: 0.7480\n",
      "Epoch 38/40\n",
      "10000/10000 [==============================] - 2s 151us/step - loss: 0.4253 - acc: 0.8277 - val_loss: 0.5853 - val_acc: 0.7470\n",
      "Epoch 39/40\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 0.4268 - acc: 0.8271 - val_loss: 0.5819 - val_acc: 0.7570\n",
      "Epoch 40/40\n",
      "10000/10000 [==============================] - 2s 161us/step - loss: 0.4181 - acc: 0.8330 - val_loss: 0.5882 - val_acc: 0.7530\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([Xstrain, Xqtrain], [Ytrain], batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS, validation_data=([Xstest, Xqtest], [Ytest]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot accuracy and loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Episodic Memory Q & A Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"r\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "#train은 상승세여도 valid는 고정, 학습을 더 시켜봐도 정확도가 오를 것 같지는 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get predictions of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.753"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ytest = np.argmax(Ytest, axis=1)\n",
    "Ytest_ = model.predict([Xstest, Xqtest])\n",
    "ytest_ = np.argmax(Ytest_, axis=1)\n",
    "accuracy_score(ytest, ytest_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Random questions and predict answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sandra moved to the bathroom . daniel travelled to the bathroom . where is daniel ? bathroom bathroom\n",
      "sandra journeyed to the hallway . mary went to the hallway . where is daniel ? office office\n",
      "sandra travelled to the bathroom . john moved to the bedroom . where is daniel ? garden garden\n",
      "mary moved to the hallway . mary moved to the bathroom . where is john ? hallway hallway\n",
      "daniel journeyed to the bathroom . daniel travelled to the office . where is daniel ? office office\n",
      "sandra went back to the bedroom . john moved to the garden . where is sandra ? bedroom bedroom\n",
      "john travelled to the bathroom . mary moved to the kitchen . where is mary ? kitchen kitchen\n",
      "john went back to the hallway . john moved to the office . where is john ? office office\n",
      "mary travelled to the bedroom . john went back to the hallway . where is mary ? bedroom bedroom\n",
      "john moved to the bedroom . john travelled to the kitchen . where is john ? kitchen kitchen\n"
     ]
    }
   ],
   "source": [
    "NUM_DISPLAY = 10\n",
    "   \n",
    "for i in random.sample(range(Xstest.shape[0]),NUM_DISPLAY):\n",
    "    story = \" \".join([indx2word[x] for x in Xstest[i].tolist() if x != 0])\n",
    "    question = \" \".join([indx2word[x] for x in Xqtest[i].tolist()])\n",
    "    label = indx2word[ytest[i]]\n",
    "    prediction = indx2word[ytest_[i]]\n",
    "    print(story, question, label, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'john moved to the bedroom . john travelled to the kitchen .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where is john ?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의미론적 영역이 포함되지 않아도, 딥러닝만으로도 어느정도 구축은 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
