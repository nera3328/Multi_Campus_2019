{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상한 나라의 엘리스 소설을 읽어온다.&전처리!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alice_in_wonderland.txt', 'r') as content_file:\n",
    "    content = content_file.read()\n",
    "#                                         특수 문자를 공백 문자로 치환\n",
    "content2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in content]).split())\n",
    " \n",
    "tokens = nltk.word_tokenize(content2)\n",
    "tokens = [word.lower() for word in tokens if len(word)>=2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3 # 3개 단어씩 토큰묶기(소설 전체 대상)\n",
    "quads = list(nltk.ngrams(tokens, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alice', 'adventures', 'in'),\n",
       " ('adventures', 'in', 'wonderland'),\n",
       " ('in', 'wonderland', 'alice')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quads[:3] # sentence 고려는 안하고 그냥 간단하게! (그냥 다음 문장으로 넘어가게 됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "newl_app = []\n",
    "for ln in quads:\n",
    "    newl = \" \".join(ln)\n",
    "    newl_app.append(newl) # 리스트화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alice adventures in', 'adventures in wonderland', 'in wonderland alice']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newl_app[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() #TFIDF아닌 카운트벡터라이저!\n",
    "\n",
    "x_trigm = []\n",
    "y_trigm = []\n",
    "\n",
    "for l in newl_app:\n",
    "    x_str = \" \".join(l.split()[0:N-1])  # trigram 중 앞 부분 2 단어. \n",
    "    y_str = l.split()[N-1]              # trigram 중 마지막 1 단어.  \n",
    "    x_trigm.append(x_str)\n",
    "    y_trigm.append(y_str)\n",
    "\n",
    "x_trigm_check = vectorizer.fit_transform(x_trigm).todense()\n",
    "y_trigm_check = vectorizer.fit_transform(y_trigm).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trigm_check[0] # 수치화 완성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries from word to integer and integer to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train shape (17947, 2559) Y Train shape (17947, 2559)\n",
      "X Test shape (7692, 2559) Y Test shape (7692, 2559)\n"
     ]
    }
   ],
   "source": [
    "dictnry = vectorizer.vocabulary_ # _ 해주면 전체 voca로 word2idx해줌\n",
    "rev_dictnry = {v:k for k,v in dictnry.items()}\n",
    "#리버스 idx2word\n",
    "X = np.array(x_trigm_check) # 수치화 완료\n",
    "Y = np.array(y_trigm_check)\n",
    "                                                    #학습, 시험 데이터 분리\n",
    "Xtrain, Xtest, Ytrain, Ytest,xtrain_tg,xtest_tg = train_test_split(X, Y,x_trigm, test_size=0.3,random_state=42)\n",
    "#진짜 notwork에 사용되는 Xtrain!\n",
    "print(\"X Train shape\",Xtrain.shape, \"Y Train shape\" , Ytrain.shape) \n",
    "# 두 단어가 섞인 []가 17947/voca 수 =2559개! 어떠한 단어가 쓰였는가 표시\n",
    "print(\"X Test shape\",Xtest.shape, \"Y Test shape\" , Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2559)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              2560000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 800)               800800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              801000    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "fourth (Dense)               (None, 2559)              2561559   \n",
      "=================================================================\n",
      "Total params: 6,723,359\n",
      "Trainable params: 6,723,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "            # input(batch_shape=(NONE, )) 배치쓰면 NONE으로 줄 수 있음! <-2D구조\n",
    "input_layer = Input(shape = (Xtrain.shape[1],))\n",
    "first_layer = Dense(1000, activation='relu')(input_layer) #히든 수 1000개!\n",
    "first_dropout = Dropout(rate = 0.5)(first_layer) # W의 50%만 버려!\n",
    "\n",
    "second_layer = Dense(800, activation='relu')(first_dropout)# 이 위로 800개 쌓음\n",
    "\n",
    "third_layer = Dense(1000, activation='relu')(second_layer) # 또 1000개 쌓음\n",
    "third_dropout = Dropout(rate = 0.5)(third_layer)\n",
    "\n",
    "fourth_layer = Dense(Ytrain.shape[1], activation='softmax')(third_dropout)\n",
    "\n",
    "\n",
    "history = Model(input_layer, fourth_layer)\n",
    "history.compile(optimizer = \"adam\",loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "print (history.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6,723,359개의 W를 update해야 한다  \n",
    "..    \n",
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 14357 samples, validate on 3590 samples\n",
      "Epoch 1/10\n",
      "14357/14357 [==============================] - 16s 1ms/step - loss: 6.3384 - acc: 0.0545 - val_loss: 6.1011 - val_acc: 0.0680\n",
      "Epoch 2/10\n",
      "14357/14357 [==============================] - 13s 897us/step - loss: 5.8985 - acc: 0.0685 - val_loss: 6.0288 - val_acc: 0.0730\n",
      "Epoch 3/10\n",
      "14357/14357 [==============================] - 13s 922us/step - loss: 5.6758 - acc: 0.0846 - val_loss: 5.9798 - val_acc: 0.0858\n",
      "Epoch 4/10\n",
      "14357/14357 [==============================] - 13s 896us/step - loss: 5.4240 - acc: 0.1066 - val_loss: 5.9822 - val_acc: 0.0933\n",
      "Epoch 5/10\n",
      "14357/14357 [==============================] - 13s 884us/step - loss: 5.1693 - acc: 0.1327 - val_loss: 5.9730 - val_acc: 0.1000\n",
      "Epoch 6/10\n",
      "14357/14357 [==============================] - 13s 897us/step - loss: 4.9345 - acc: 0.1507 - val_loss: 5.9924 - val_acc: 0.1047\n",
      "Epoch 7/10\n",
      "14357/14357 [==============================] - 13s 888us/step - loss: 4.7026 - acc: 0.1781 - val_loss: 6.0652 - val_acc: 0.1078\n",
      "Epoch 8/10\n",
      "14357/14357 [==============================] - 13s 889us/step - loss: 4.4771 - acc: 0.2018 - val_loss: 6.1677 - val_acc: 0.1095\n",
      "Epoch 9/10\n",
      "14357/14357 [==============================] - 13s 871us/step - loss: 4.2711 - acc: 0.2209 - val_loss: 6.2687 - val_acc: 0.1086\n",
      "Epoch 10/10\n",
      "14357/14357 [==============================] - 13s 879us/step - loss: 4.0825 - acc: 0.2422 - val_loss: 6.3457 - val_acc: 0.1103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x70711c50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = history.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample check on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior bigram words | Actual | Predicted \n",
      "\n",
      "0 the evening | beautiful | beautiful  turtle  rabbit  hall  little  \n",
      "1 slipped in | like | and  in  the  with  half  \n",
      "2 alice swallowing | down | good  her  very  little  not  \n",
      "3 an encouraging | tone | of  to  but  again  voice  \n",
      "4 waistcoat pocket | or | went  music  and  sat  repeated  \n",
      "5 she went | on | on  down  back  hunting  up  \n",
      "6 that she | knew | was  had  would  got  said  \n",
      "7 down on | her | one  the  his  their  her  \n",
      "8 dormouse went | on | on  down  after  without  back  \n",
      "9 soup soup | of | you  said  and  to  in  \n"
     ]
    }
   ],
   "source": [
    "print (\"Prior bigram words\",\"| Actual\",\"| Predicted\",\"\\n\")\n",
    "#확률 순으로 추천단어 5개 추출로 뽑아주게 변경\n",
    "for i in range(10):\n",
    "    print (i,xtest_tg[i], \"|\", rev_dictnry[np.argmax(Ytest[i])], \"| \", end='')\n",
    "    idx = np.flipud(Y_pred[i].argsort())[:5]\n",
    "    for k in idx:\n",
    "        print(rev_dictnry[k], ' ', end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
