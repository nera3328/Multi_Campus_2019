{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시험을 위해 일부 데이터만 사용해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = newsgroups_train.data\n",
    "x_test = newsgroups_test.data\n",
    "\n",
    "y_train = newsgroups_train.target #20개 카테고리로 클래스가 분류됨\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all 20 categories:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "\n",
      "Sample Email:\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sample Target Category:\n",
      "7\n",
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "print (\"List of all 20 categories:\")\n",
    "print (newsgroups_train.target_names)\n",
    "print (\"\\n\")\n",
    "print (\"Sample Email:\")\n",
    "print (x_train[0])\n",
    "print (\"Sample Target Category:\")\n",
    "print (y_train[0])\n",
    "print (newsgroups_train.target_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used for pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords #불용어\n",
    "from nltk.stem import WordNetLemmatizer #형태소로 어근에 맞추어 단어 분류\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag #lemma처리 해도 working과 같이 명사, 동사로 쓰이는 경우가 있으니 태그로 처리할거야\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text): #email 문서 1개\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "                                            #특수 문자를 공백 문자로 처리하겠다.\n",
    "    #text2 = ''\n",
    "    #for ch in text:\n",
    "    #    if ch in string.punctuation:\n",
    "    #        text2 += ' '\n",
    "    #    else:\n",
    "    #        text2 += ch\n",
    "    #text2 = ' '.join(text2.split())  # '\\n'을 제거하기 위해 split() 후 다시 합침\n",
    "    \n",
    "                                                 #Email 1개 문서\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    #tokens = []\n",
    "    #for sent in nltk.sent_tokenize(text2):\n",
    "    #    for word in nltk.word_tokenize(sent):\n",
    "    #        tokens.append(word.lower())\n",
    "            \n",
    "        #sent: email 1개의 1문장\n",
    "        #단어단위로 쪼개서 word안에 넣어\n",
    "        #word를 소문자로 token이라는 list에 집어넣어\n",
    "        \n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    \n",
    "    #stemmer = PorterStemmer() #어간 추출\n",
    "    #try:\n",
    "    #    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    #except:\n",
    "    #    tokens = tokens #혹시나 잘 안되면 냅둬라\n",
    "        \n",
    "    tagged_corpus = pos_tag(tokens)    #품사에 따라 태그 붙이기. 얘는 nltk에 내장되어 있다.\n",
    "    \n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer() #아래 것들을 얘가 해주는거\n",
    "\n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n') #token = working\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v') #token = work\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "    \n",
    "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus]) #함수 호출, 함수 안 함수\n",
    "                                #여기서 불렀고         태그 붙인 것에 따라 하나씩 pre_proc_text에 넣어 \n",
    "    return pre_proc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rid'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('riding','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed  = [] #전처리가 완료된 학습 시킬 데이터\n",
    "for i in x_train:\n",
    "    x_train_preprocessed.append(preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_preprocessed = []\n",
    "for i in x_test:\n",
    "    x_test_preprocessed.append(preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lerxst wam umd edu thing subject car nntp post host rac3 wam umd edu organization university maryland college park line wonder anyone could enlighten car saw day door sport car look late 60 early 70 call bricklin door really small addition front bumper separate rest body know anyone tellme model name engine spec year production car make history whatever info funky look car please mail thanks bring neighborhood lerxst'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_preprocessed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### building TFIDF vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2),  stop_words='english', max_features= 10000)\n",
    "    #적어도 최소 2번 이상 쓰인 단어만 쓴다.      (unigram, bigram) 이미 sklearn에 stopword 내장, 가장 빈도 높은 것으로 10000개를 끊어 사용하겠다.\n",
    "\n",
    "        # = vectorizer.fit(x_train_preprocessed)-------tf x idf\n",
    "        # = vectorizer.transform(x+train_processed).todense()\n",
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n",
    "#         [                         tf, idf               ] todense : 여기까지 CSR(행렬)로 리턴해줌.\n",
    "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()\n",
    "                                #=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_2[0]  #띄엄띄엄있다 -> sparse matrix(내부적으로 CSR format 가지고 있음.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_2[0].shape           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names()) #voca의 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta,Adam,RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definiting hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337) \n",
    "nb_classes = 20 \n",
    "batch_size = 64 #mini-batch\n",
    "nb_epochs = 200\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes) #one-hot으로 변환 (인공신경망 동작원리)\n",
    "                                #y_train을 넣고 20개로 변환시켜라\n",
    "\n",
    "#        y-train이 0이면 첫 번째만 1로 활성화시키고 나며지는 0으로 하겠다. \n",
    "#           0->[1,0,0,0,0,0,,,,,0,0]\n",
    "#          1->[0,1,0,0,0,0,,,,0,0] 이런식으로 20개 총 19번까지 빼내겠다  \n",
    "\n",
    "# 물론 2진법으로 변환해서 19=10011-> [1,0,0,1,1]이런식으로 넣을 수도 있음  (얘가 메모리는 더 작음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(11314, 20)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0])\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Layer Model building in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0712 13:29:41.183151  5396 deprecation_wrapper.py:119] From C:\\Anaconda3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0712 13:29:41.477168  5396 deprecation_wrapper.py:119] From C:\\Anaconda3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0712 13:29:41.536171  5396 deprecation_wrapper.py:119] From C:\\Anaconda3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0712 13:29:41.658178  5396 deprecation_wrapper.py:119] From C:\\Anaconda3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0712 13:29:41.693180  5396 deprecation.py:506] From C:\\Anaconda3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0712 13:29:41.745183  5396 deprecation_wrapper.py:119] From C:\\Anaconda3.7\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0712 13:29:41.769185  5396 deprecation_wrapper.py:119] From C:\\Anaconda3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              10001000  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                20020     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 10,021,020\n",
      "Trainable params: 10,021,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1000, input_shape= (x_train_2.shape[1],)))\n",
    "                                    #2000행(email 개수), shape[1]=10000개( feature는 단어 만개만 쓰기로 했으니까!)\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))#activationFunction은 softmax! 다중분류니까                           sigmoid도 가능 근데 이러면\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam') #lossfuction은 크로스 엔트로피! 확률분포니까         얘는 MSE써\n",
    "print (model.summary())\n",
    "                                                    #10,000 x 1,000개 +1000개(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 13:29:42.089203  5396 deprecation.py:323] From C:\\Anaconda3.7\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11314/11314 [==============================] - 31s 3ms/step - loss: 1.5118\n",
      "Epoch 2/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.2931\n",
      "Epoch 3/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.1159\n",
      "Epoch 4/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0556\n",
      "Epoch 5/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0303\n",
      "Epoch 6/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0196\n",
      "Epoch 7/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0137\n",
      "Epoch 8/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0109\n",
      "Epoch 9/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0089\n",
      "Epoch 10/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0072\n",
      "Epoch 11/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0063\n",
      "Epoch 12/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0059\n",
      "Epoch 13/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0043\n",
      "Epoch 14/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0042\n",
      "Epoch 15/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0041\n",
      "Epoch 16/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0036\n",
      "Epoch 17/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0042\n",
      "Epoch 18/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0034\n",
      "Epoch 19/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0026\n",
      "Epoch 20/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0033\n",
      "Epoch 21/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0022\n",
      "Epoch 22/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 23/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0024\n",
      "Epoch 24/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0034\n",
      "Epoch 25/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 26/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0034\n",
      "Epoch 27/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 28/200\n",
      "11314/11314 [==============================] - 30s 3ms/step - loss: 0.0026\n",
      "Epoch 29/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0028\n",
      "Epoch 30/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0023\n",
      "Epoch 31/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 32/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0021\n",
      "Epoch 33/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0022\n",
      "Epoch 34/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0031\n",
      "Epoch 35/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0023\n",
      "Epoch 36/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0033\n",
      "Epoch 37/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 38/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0021\n",
      "Epoch 39/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0024\n",
      "Epoch 40/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0024\n",
      "Epoch 41/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0029\n",
      "Epoch 42/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 43/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0026\n",
      "Epoch 44/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0027\n",
      "Epoch 45/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0017\n",
      "Epoch 46/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0040\n",
      "Epoch 47/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0032\n",
      "Epoch 48/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0040\n",
      "Epoch 49/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0010\n",
      "Epoch 50/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0035\n",
      "Epoch 51/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0022\n",
      "Epoch 52/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0026\n",
      "Epoch 53/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 54/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0018\n",
      "Epoch 55/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0034\n",
      "Epoch 56/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0016\n",
      "Epoch 57/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0024\n",
      "Epoch 58/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 59/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0018\n",
      "Epoch 60/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 61/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 62/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0013\n",
      "Epoch 63/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0024\n",
      "Epoch 64/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0016\n",
      "Epoch 65/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0014\n",
      "Epoch 66/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 67/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0019\n",
      "Epoch 68/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0038\n",
      "Epoch 69/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 6.4090e-04\n",
      "Epoch 70/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0023\n",
      "Epoch 71/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0016\n",
      "Epoch 72/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0031\n",
      "Epoch 73/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0016\n",
      "Epoch 74/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0026\n",
      "Epoch 75/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0027\n",
      "Epoch 76/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0024\n",
      "Epoch 77/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0038\n",
      "Epoch 78/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0021\n",
      "Epoch 79/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 80/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 81/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 82/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0023\n",
      "Epoch 83/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0018\n",
      "Epoch 84/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 85/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0027\n",
      "Epoch 86/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0029\n",
      "Epoch 87/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 9.2012e-04\n",
      "Epoch 88/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0019\n",
      "Epoch 89/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 90/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0022\n",
      "Epoch 91/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0024\n",
      "Epoch 92/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0021\n",
      "Epoch 93/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0018\n",
      "Epoch 94/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0030\n",
      "Epoch 95/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0017\n",
      "Epoch 96/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 97/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0022\n",
      "Epoch 98/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0014\n",
      "Epoch 99/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0015\n",
      "Epoch 100/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0020\n",
      "Epoch 101/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0015\n",
      "Epoch 102/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0035\n",
      "Epoch 103/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0010\n",
      "Epoch 104/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 9.8757e-04\n",
      "Epoch 105/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0018\n",
      "Epoch 106/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0011\n",
      "Epoch 107/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0014\n",
      "Epoch 108/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0022\n",
      "Epoch 109/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0012\n",
      "Epoch 110/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 111/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0018\n",
      "Epoch 112/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 8.7333e-04\n",
      "Epoch 113/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 114/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0011\n",
      "Epoch 115/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0015\n",
      "Epoch 116/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0014\n",
      "Epoch 117/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0030\n",
      "Epoch 118/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0016\n",
      "Epoch 119/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 120/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0019\n",
      "Epoch 121/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 122/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0017\n",
      "Epoch 123/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0028\n",
      "Epoch 124/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0014\n",
      "Epoch 125/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0029\n",
      "Epoch 126/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 127/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0014\n",
      "Epoch 128/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0019\n",
      "Epoch 129/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0024\n",
      "Epoch 130/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0018\n",
      "Epoch 131/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 132/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0015\n",
      "Epoch 133/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 5.5436e-04\n",
      "Epoch 134/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0021\n",
      "Epoch 135/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0017\n",
      "Epoch 136/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0029\n",
      "Epoch 137/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0016\n",
      "Epoch 138/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0010\n",
      "Epoch 139/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0011\n",
      "Epoch 140/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 141/200\n",
      "11314/11314 [==============================] - 30s 3ms/step - loss: 0.0027\n",
      "Epoch 142/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 4.4284e-04\n",
      "Epoch 143/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0013\n",
      "Epoch 144/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0019\n",
      "Epoch 145/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 146/200\n",
      "11314/11314 [==============================] - 30s 3ms/step - loss: 0.0013\n",
      "Epoch 147/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 148/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0017\n",
      "Epoch 149/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 9.4168e-04\n",
      "Epoch 150/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0014\n",
      "Epoch 151/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0024\n",
      "Epoch 152/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 5.8067e-04\n",
      "Epoch 153/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0023\n",
      "Epoch 154/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0013\n",
      "Epoch 155/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 156/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0022\n",
      "Epoch 157/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0017\n",
      "Epoch 158/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0014\n",
      "Epoch 159/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0021\n",
      "Epoch 160/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 8.8231e-04\n",
      "Epoch 161/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0013\n",
      "Epoch 162/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0018\n",
      "Epoch 163/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0017\n",
      "Epoch 164/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 0.0023\n",
      "Epoch 165/200\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 7.6524e-04\n",
      "Epoch 166/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0027\n",
      "Epoch 167/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 168/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0014\n",
      "Epoch 169/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0010\n",
      "Epoch 170/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 6.6666e-04\n",
      "Epoch 171/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0017\n",
      "Epoch 172/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 173/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0013\n",
      "Epoch 174/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0017\n",
      "Epoch 175/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0021\n",
      "Epoch 176/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0018\n",
      "Epoch 177/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0018\n",
      "Epoch 178/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0012\n",
      "Epoch 179/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 4.8450e-04\n",
      "Epoch 180/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0022\n",
      "Epoch 181/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 6.1492e-04\n",
      "Epoch 182/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 2.7752e-04\n",
      "Epoch 183/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 2.0168e-04\n",
      "Epoch 184/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0025\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0017\n",
      "Epoch 186/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0012\n",
      "Epoch 187/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0029\n",
      "Epoch 188/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 189/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0020\n",
      "Epoch 190/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0016\n",
      "Epoch 191/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 9.2052e-04\n",
      "Epoch 192/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0019\n",
      "Epoch 193/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 6.1801e-04\n",
      "Epoch 194/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0012\n",
      "Epoch 195/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0013\n",
      "Epoch 196/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0021\n",
      "Epoch 197/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0028\n",
      "Epoch 198/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0015\n",
      "Epoch 199/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 9.5879e-04\n",
      "Epoch 200/200\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d2ff320>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.818\n"
     ]
    }
   ],
   "source": [
    "y_train_predclass = model.predict_classes(x_train_2,batch_size=batch_size)\n",
    "y_test_predclass = model.predict_classes(x_test_2,batch_size=batch_size)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print (\"Train accuracy:\", np.round(accuracy_score(y_train,y_train_predclass), 3))\n",
    "print (\"Test accuracy:\", np.round(accuracy_score(y_test,y_test_predclass), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
