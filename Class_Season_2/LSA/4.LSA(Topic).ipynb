{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA (Latent Semantic Analysis)를 이용한 뉴스 데이터 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re #정규 표현식 사용\n",
    "import pickle#자료형 변경없이 불러올 수 있음\n",
    "from nltk.corpus import stopwords\n",
    "#from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newsData = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./data/news.data', 'wb') as f:\n",
    "#    pickle.dump(newsData , f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주석 애들 처리하면 직접 파일 받아옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./news.data', 'rb') as f:\n",
    "    newsData  = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "Well i'm not sure about the story nad it did seem biased. What\n",
      "I disagree with is your statement that the U.S. Media is out to\n",
      "ruin Israels reputation. That is rediculous. The U.S. media is\n",
      "the most pro-israeli media in the world. Having lived in Europe\n",
      "I realize that incidences such as the one described in the\n",
      "letter have occured. The U.S. media as a whole seem to try to\n",
      "ignore them. The U.S. is subsidizing Israels existance and the\n",
      "Europeans are not (at least not to the same degree). So I think\n",
      "that might be a reason they report more clearly on the\n",
      "atrocities.\n",
      "\tWhat is a shame is that in Austria, daily reports of\n",
      "the inhuman acts commited by Israeli soldiers and the blessing\n",
      "received from the Government makes some of the Holocaust guilt\n",
      "go away. After all, look how the Jews are treating other races\n",
      "when they got power. It is unfortunate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news = newsData.data\n",
    "print(len(news))\n",
    "print(news[0])#첫 번째 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(newsData.target_names)\n",
    "print(len(newsData.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing\n",
    "영문자가 아닌 문자를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news1 = []\n",
    "for doc in news: #뉴스 하나씩 읽어\n",
    "    news1.append(re.sub(\"[^a-zA-Z]\", \" \", doc)) #^: start  영문자 이외의 문자는 공백\n",
    " # ^ : start  + : 전 문자가 1개 이상 / * : 0개 이상 /{2, 5} : 2~5번 반복 / $ : end / [a-z] : a-z 중 한 개\n",
    "                                                                 # [^a-z] : ^가 not으로 바뀜 --> a-z아닌것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어를 제거하고, 모든 단어를 소문자로 변환하고, 길이가 3 이하인 단어를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "news2 = []\n",
    "for doc in news1:\n",
    "    doc1 = []\n",
    "    for w in doc.split():\n",
    "        w = w.lower()\n",
    "        if len(w) > 3 and w not in stop_words:\n",
    "            doc1.append(w)\n",
    "    news2.append(' '.join(doc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well sure story seem biased disagree statement media ruin israels reputation rediculous media israeli media world lived europe realize incidences described letter occured media whole seem ignore subsidizing israels existance europeans least degree think might reason report clearly atrocities shame austria daily reports inhuman acts commited israeli soldiers blessing received government makes holocaust guilt away look jews treating races power unfortunate\n"
     ]
    }
   ],
   "source": [
    "print(news2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF matrix 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vector = TfidfVectorizer(max_features = 500) #객체 생성, 사용 단어 500개 한정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf_vector.fit_transform(news2) #500개까지 쳐줘서 tfidf 구성하겠다/ fit : vocabulary 생성, transform\n",
    "# Term Document Matrix--> 여기서는 DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 500)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.shape) #     (1, 500) : 문서 1개의 tfidf   X11314\n",
    "#print(tfidf[0].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_vector.get_feature_names() #voca 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'access', 'actually', 'address', 'advance', 'agree', 'allow', 'almost', 'already', 'also', 'although', 'always', 'american', 'among', 'anonymous', 'another', 'answer', 'anti', 'anybody', 'anyone']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 끝!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis (LSA) :(U S V^t분해!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD #주제 개수 만큼 S행렬을 자른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = len(newsData.target_names), n_iter=100) \n",
    "                #(20개로 임의로 값을 줌, 100 -수치 반복)\n",
    "     #      주제 개수 =        20            반복        topic 주제 수는 정해진 값이 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.fit(tfidf) #분해!  fit시키면 tfidf 애들이 U S Vt에 대한 정보가 output으로 나온다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U, S, VT 행렬\n",
    "U 행렬 ~ 차원 = (문서 개수 X topic 개수) : 문서당 topic 분포<br>\n",
    "S 행렬 ~ 차원 = (topic 개수 X topic 개수)<br>\n",
    "VT 행렬. 차원 = (topic 개수 X 단어 개수) : topic 당 단어 빈도 (분포)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = svd.fit_transform(tfidf) / svd.singular_values_\n",
    "U.shape\n",
    "#(문서갯수 , 주제 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT = svd.components_\n",
    "VT.shape\n",
    "#(주제 개수, 단어 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.diag(svd.singular_values_)\n",
    "S.shape\n",
    "#(주제 수 , )  truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic별로 문서 분류\n",
    "U 행렬에서 가장 큰 colume을 선택한다. Colume에 topic 값이 부여돼 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00834054, -0.01091546, -0.00328674, -0.00442068, -0.00456436,\n",
       "         0.00239253,  0.00031934, -0.00100528,  0.00407314, -0.00194108,\n",
       "        -0.00194613,  0.0092436 ,  0.00233742, -0.00271065, -0.00185346,\n",
       "         0.00304093, -0.00786485,  0.01936485, -0.01445553,  0.00790409]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[0:1, :] # U : (문서개수(20개), topic개수) \n",
    "# - 값에 대한 의문? 왜나오는가(모호)----->LSD는 이러한 모호함 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서-1 : topic = 18\n",
      "문서-2 : topic = 1\n",
      "문서-3 : topic = 18\n",
      "문서-4 : topic = 6\n",
      "문서-5 : topic = 4\n",
      "문서-6 : topic = 9\n",
      "문서-7 : topic = 1\n",
      "문서-8 : topic = 7\n",
      "문서-9 : topic = 8\n",
      "문서-10 : topic = 13\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): #10개 문서만 보자\n",
    "    print('문서-{:d} : topic = {:d}'.format(i+1, np.argmax(U[i:(i+1), :][0])+1))\n",
    "# U 행렬을 통해 문서를 주제 별로 분류 할 수 있다\n",
    "\n",
    "# 문서 1과 문서 2는 같은 주제를 다루는 NEWS다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VT 행렬에서 topic 별로 중요 단어를 표시한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 500)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽- 1 : would like know people think good also could time well \n",
      "토픽- 2 : thanks windows please anyone mail card know advance drive file \n",
      "토픽- 3 : would thanks anyone know like please could mail someone advance \n",
      "토픽- 4 : game team year games good last season players play hockey \n",
      "토픽- 5 : would like drive system windows card scsi disk team problem \n",
      "토픽- 6 : drive please scsi hard mail sale would email drives people \n",
      "토픽- 7 : drive know like anyone scsi drives hard something card think \n",
      "토픽- 8 : like please sale mail email offer something send list interested \n",
      "토픽- 9 : think windows people please card jesus thanks believe bible mail \n",
      "토픽-10 : good card think sale price bike also much looking offer \n",
      "토픽-11 : card people video know sale monitor government drivers price offer \n",
      "토픽-12 : think chip system could encryption clipper need government space much \n",
      "토픽-13 : could thanks right card problem much bike well someone advance \n",
      "토픽-14 : good people windows file government files thanks drive would year \n",
      "토픽-15 : anyone thanks like good also people space card could system \n",
      "토픽-16 : space year thanks problem nasa much bike also time think \n",
      "토픽-17 : problem thanks system game need window jesus time first using \n",
      "토픽-18 : anyone right israel think sale window problem israeli government back \n",
      "토픽-19 : problem people please anyone could system email problems good time \n",
      "토픽-20 : also year good know window problem please israel last using \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(VT)):\n",
    "    idx = np.flipud(VT[i].argsort())[:10] # argsort : 작은 것 부터 순서대로 문서 반환, flipud : 배열 위아래 뒤집 \n",
    "    print('토픽-{:2d} : '.format(i+1), end='')\n",
    "    for n in idx:\n",
    "        print('{:s} '.format(vocab[n]), end='')\n",
    "    print()\n",
    "    \n",
    "    #중요한 순으로 문장의 단어 나열\n",
    "    #전처리가 잘 되지는 않음. would like know 이런 애들은 불용어 처리 해 주어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well i'm not sure about the story nad it did seem biased. What\n",
      "I disagree with is your statement that the U.S. Media is out to\n",
      "ruin Israels reputation. That is rediculous. The U.S. media is\n",
      "the most pro-israeli media in the world. Having lived in Europe\n",
      "I realize that incidences such as the one described in the\n",
      "letter have occured. The U.S. media as a whole seem to try to\n",
      "ignore them. The U.S. is subsidizing Israels existance and the\n",
      "Europeans are not (at least not to the same degree). So I think\n",
      "that might be a reason they report more clearly on the\n",
      "atrocities.\n",
      "\tWhat is a shame is that in Austria, daily reports of\n",
      "the inhuman acts commited by Israeli soldiers and the blessing\n",
      "received from the Government makes some of the Holocaust guilt\n",
      "go away. After all, look how the Jews are treating other races\n",
      "when they got power. It is unfortunate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although I realize that principle is not one of your strongest\n",
      "points, I would still like to know why do do not ask any question\n",
      "of this sort about the Arab countries.\n",
      "\n",
      "   If you want to continue this think tank charade of yours, your\n",
      "fixation on Israel must stop.  You might have to start asking the\n",
      "same sort of questions of Arab countries as well.  You realize it\n",
      "would not work, as the Arab countries' treatment of Jews over the\n",
      "last several decades is so bad that your fixation on Israel would\n",
      "begin to look like the biased attack that it is.\n",
      "\n",
      "   Everyone in this group recognizes that your stupid 'Center for\n",
      "Policy Research' is nothing more than a fancy name for some bigot\n",
      "who hates Israel.\n"
     ]
    }
   ],
   "source": [
    "print(news[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A = U x S x VT\n",
    "이 값은 tfidf matrix에 SVD가 적용된 행렬이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.dot(U, np.dot(S, VT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 500)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 500)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00671794,  0.00212366,  0.01166669, ...,  0.01825244,\n",
       "         0.02951324,  0.02430763],\n",
       "       [ 0.01198476,  0.00455351,  0.01565654, ...,  0.01271674,\n",
       "        -0.01521117, -0.00296277],\n",
       "       [ 0.01327301,  0.00209872,  0.01144589, ...,  0.01295922,\n",
       "         0.0428024 ,  0.03125753],\n",
       "       ...,\n",
       "       [ 0.00355683, -0.00235559,  0.00711601, ...,  0.00302724,\n",
       "         0.00996596,  0.00567007],\n",
       "       [ 0.01201571,  0.02172375,  0.00327202, ..., -0.00319135,\n",
       "        -0.09744651, -0.02149235],\n",
       "       [ 0.0133031 ,  0.002536  ,  0.02125163, ...,  0.0140308 ,\n",
       "         0.11475994,  0.0528168 ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.17877864, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.13438696, ..., 0.        , 0.12961864,\n",
       "        0.24839851]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문서 유사도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문서-1과 문서-2의 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26719502]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(A[0:1, :], A[1:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01992352]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf.toarray()[0:1, :], tfidf.toarray()[1:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문서-1과 문서-3의 유사도\n",
    "문서-1은 문서-2보다 문서-3과 더 유사하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69869725]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(A[0:1, :], A[2:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11930785]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf.toarray()[0:1, :], tfidf.toarray()[2:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q) U와 VT에 있는 음수 값은 무엇을 의미하는가?\n",
    "참고 : Non-negative Matrix Factorisation (NMF)<br>\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
