{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)를 이용한 뉴스 데이터 클러스터링\n",
    "sklearn 패키지를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "#from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newsData = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./data/news.data', 'wb') as f:\n",
    "#    pickle.dump(newsData , f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/news.data', 'rb') as f:\n",
    "    newsData  = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "Well i'm not sure about the story nad it did seem biased. What\n",
      "I disagree with is your statement that the U.S. Media is out to\n",
      "ruin Israels reputation. That is rediculous. The U.S. media is\n",
      "the most pro-israeli media in the world. Having lived in Europe\n",
      "I realize that incidences such as the one described in the\n",
      "letter have occured. The U.S. media as a whole seem to try to\n",
      "ignore them. The U.S. is subsidizing Israels existance and the\n",
      "Europeans are not (at least not to the same degree). So I think\n",
      "that might be a reason they report more clearly on the\n",
      "atrocities.\n",
      "\tWhat is a shame is that in Austria, daily reports of\n",
      "the inhuman acts commited by Israeli soldiers and the blessing\n",
      "received from the Government makes some of the Holocaust guilt\n",
      "go away. After all, look how the Jews are treating other races\n",
      "when they got power. It is unfortunate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news = newsData.data\n",
    "print(len(news))\n",
    "print(news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(newsData.target_names)\n",
    "print(len(newsData.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing\n",
    "영문자가 아닌 문자를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news1 = []\n",
    "for doc in news:\n",
    "    news1.append(re.sub(\"[^a-zA-Z]\", \" \", doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어를 제거하고, 모든 단어를 소문자로 변환하고, 길이가 3 이하인 단어를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "news2 = []\n",
    "for doc in news1:\n",
    "    doc1 = []\n",
    "    for w in doc.split():\n",
    "        w = w.lower()\n",
    "        if len(w) > 3 and w not in stop_words:\n",
    "            doc1.append(w)\n",
    "    news2.append(' '.join(doc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well sure story seem biased disagree statement media ruin israels reputation rediculous media israeli media world lived europe realize incidences described letter occured media whole seem ignore subsidizing israels existance europeans least degree think might reason report clearly atrocities shame austria daily reports inhuman acts commited israeli soldiers blessing received government makes holocaust guilt away look jews treating races power unfortunate\n"
     ]
    }
   ],
   "source": [
    "print(news2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF matrix 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vector = TfidfVectorizer(max_features = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf_vector.fit_transform(news2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 500)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.shape)\n",
    "#print(tfidf[0].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'access', 'actually', 'address', 'advance', 'agree', 'allow', 'almost', 'already', 'also', 'although', 'always', 'american', 'among', 'anonymous', 'another', 'answer', 'anti', 'anybody', 'anyone']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LDA(n_components = len(newsData.target_names), learning_method='online', \n",
    "            evaluate_every=5, max_iter=100, verbose=1)   #학습방법은 batch / online(그때 그때 작업) 중 선택\n",
    "                        #target_name=20개로 한정,  verbose : 상황 보여줘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting and transform\n",
    "- Return 값이 Document-Topic distribution이다.\n",
    "- iteration 횟수가 max_iter까지 가면 아직 수렴하지 않은 것이다.\n",
    "- 아직 수렴하지 않은 경우 mat_iter를 증가시켜야 한다.\n",
    "- mat_iter를 증가시켜도 수렴하지 못하는 경우는 preprocessing 등을 좀 더 정밀하게 해야 한다.\n",
    "- perplexity(내부 평가 지표)는 감소해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 100\n",
      "iteration: 2 of max_iter: 100\n",
      "iteration: 3 of max_iter: 100\n",
      "iteration: 4 of max_iter: 100\n",
      "iteration: 5 of max_iter: 100, perplexity: 1560.9814\n",
      "iteration: 6 of max_iter: 100\n",
      "iteration: 7 of max_iter: 100\n",
      "iteration: 8 of max_iter: 100\n",
      "iteration: 9 of max_iter: 100\n",
      "iteration: 10 of max_iter: 100, perplexity: 1565.6185\n",
      "iteration: 11 of max_iter: 100\n",
      "iteration: 12 of max_iter: 100\n",
      "iteration: 13 of max_iter: 100\n",
      "iteration: 14 of max_iter: 100\n",
      "iteration: 15 of max_iter: 100, perplexity: 1565.8587\n",
      "iteration: 16 of max_iter: 100\n",
      "iteration: 17 of max_iter: 100\n",
      "iteration: 18 of max_iter: 100\n",
      "iteration: 19 of max_iter: 100\n",
      "iteration: 20 of max_iter: 100, perplexity: 1569.6092\n",
      "iteration: 21 of max_iter: 100\n",
      "iteration: 22 of max_iter: 100\n",
      "iteration: 23 of max_iter: 100\n",
      "iteration: 24 of max_iter: 100\n",
      "iteration: 25 of max_iter: 100, perplexity: 1571.1352\n",
      "iteration: 26 of max_iter: 100\n",
      "iteration: 27 of max_iter: 100\n",
      "iteration: 28 of max_iter: 100\n",
      "iteration: 29 of max_iter: 100\n",
      "iteration: 30 of max_iter: 100, perplexity: 1572.3014\n",
      "iteration: 31 of max_iter: 100\n",
      "iteration: 32 of max_iter: 100\n",
      "iteration: 33 of max_iter: 100\n",
      "iteration: 34 of max_iter: 100\n",
      "iteration: 35 of max_iter: 100, perplexity: 1574.9264\n",
      "iteration: 36 of max_iter: 100\n",
      "iteration: 37 of max_iter: 100\n",
      "iteration: 38 of max_iter: 100\n",
      "iteration: 39 of max_iter: 100\n",
      "iteration: 40 of max_iter: 100, perplexity: 1575.0877\n",
      "iteration: 41 of max_iter: 100\n",
      "iteration: 42 of max_iter: 100\n",
      "iteration: 43 of max_iter: 100\n",
      "iteration: 44 of max_iter: 100\n",
      "iteration: 45 of max_iter: 100, perplexity: 1575.3919\n",
      "iteration: 46 of max_iter: 100\n",
      "iteration: 47 of max_iter: 100\n",
      "iteration: 48 of max_iter: 100\n",
      "iteration: 49 of max_iter: 100\n",
      "iteration: 50 of max_iter: 100, perplexity: 1574.9472\n",
      "iteration: 51 of max_iter: 100\n",
      "iteration: 52 of max_iter: 100\n",
      "iteration: 53 of max_iter: 100\n",
      "iteration: 54 of max_iter: 100\n",
      "iteration: 55 of max_iter: 100, perplexity: 1575.1216\n",
      "iteration: 56 of max_iter: 100\n",
      "iteration: 57 of max_iter: 100\n",
      "iteration: 58 of max_iter: 100\n",
      "iteration: 59 of max_iter: 100\n",
      "iteration: 60 of max_iter: 100, perplexity: 1574.7376\n",
      "iteration: 61 of max_iter: 100\n",
      "iteration: 62 of max_iter: 100\n",
      "iteration: 63 of max_iter: 100\n",
      "iteration: 64 of max_iter: 100\n",
      "iteration: 65 of max_iter: 100, perplexity: 1575.3386\n",
      "iteration: 66 of max_iter: 100\n",
      "iteration: 67 of max_iter: 100\n",
      "iteration: 68 of max_iter: 100\n",
      "iteration: 69 of max_iter: 100\n",
      "iteration: 70 of max_iter: 100, perplexity: 1575.9373\n",
      "iteration: 71 of max_iter: 100\n",
      "iteration: 72 of max_iter: 100\n",
      "iteration: 73 of max_iter: 100\n",
      "iteration: 74 of max_iter: 100\n",
      "iteration: 75 of max_iter: 100, perplexity: 1575.8196\n",
      "iteration: 76 of max_iter: 100\n",
      "iteration: 77 of max_iter: 100\n",
      "iteration: 78 of max_iter: 100\n",
      "iteration: 79 of max_iter: 100\n",
      "iteration: 80 of max_iter: 100, perplexity: 1575.7119\n",
      "iteration: 81 of max_iter: 100\n",
      "iteration: 82 of max_iter: 100\n",
      "iteration: 83 of max_iter: 100\n",
      "iteration: 84 of max_iter: 100\n",
      "iteration: 85 of max_iter: 100, perplexity: 1575.1223\n",
      "iteration: 86 of max_iter: 100\n",
      "iteration: 87 of max_iter: 100\n",
      "iteration: 88 of max_iter: 100\n",
      "iteration: 89 of max_iter: 100\n",
      "iteration: 90 of max_iter: 100, perplexity: 1575.4182\n",
      "iteration: 91 of max_iter: 100\n",
      "iteration: 92 of max_iter: 100\n",
      "iteration: 93 of max_iter: 100\n",
      "iteration: 94 of max_iter: 100\n",
      "iteration: 95 of max_iter: 100, perplexity: 1574.4128\n",
      "iteration: 96 of max_iter: 100\n",
      "iteration: 97 of max_iter: 100\n",
      "iteration: 98 of max_iter: 100\n",
      "iteration: 99 of max_iter: 100\n",
      "iteration: 100 of max_iter: 100, perplexity: 1576.3092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11314, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lsa의 U에 해당\n",
    "doc_topic = model.fit_transform(tfidf)\n",
    "doc_topic.shape\n",
    "#(수렴안함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic-Term distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 500)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#어떤 단어가 어느 주제에 많이 있나에 대한 분포\n",
    "\n",
    "topic_term = model.components_ \n",
    "topic_term.shape\n",
    "#(주제(토픽), 워드)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic별로 문서 분류\n",
    "doc_topic 행렬에서 가장 큰 colume을 선택한다. Colume에 topic의 score가 부여돼 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01044057, 0.1126237 , 0.01044057, 0.01044057, 0.01044057,\n",
       "        0.01044057, 0.01044057, 0.01044057, 0.01044057, 0.01044057,\n",
       "        0.01044057, 0.01044057, 0.01044057, 0.01044057, 0.17293108,\n",
       "        0.01044057, 0.01044057, 0.01044057, 0.01044057, 0.5369555 ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic[0:1, :]#문서 1에서 눈에 띄게 큰 17번째 값 주목\n",
    "# LSA : - 값에 대한 의문? 왜나오는가(모호)----->LSD는 이러한 모호함 X 다들 양수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서-1 : topic = 20\n",
      "문서-2 : topic = 16\n",
      "문서-3 : topic = 16\n",
      "문서-4 : topic = 5\n",
      "문서-5 : topic = 16\n",
      "문서-6 : topic = 16\n",
      "문서-7 : topic = 20\n",
      "문서-8 : topic = 20\n",
      "문서-9 : topic = 17\n",
      "문서-10 : topic = 20\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('문서-{:d} : topic = {:d}'.format(i+1, np.argmax(doc_topic[i:(i+1), :][0])+1))\n",
    "             # ex) 0: 1까지 = 0을 읽겠다(하나씩 읽겠다), : 끝까지!, [0]=2차원이니까 차원 축소,  사람이 보게 +1 !   \n",
    "# 큰 값에 해당시켜 나열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### topic_term 행렬에서 topic 별로 중요 단어를 표시한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽- 1 : back phone remember internet rather little difference either actually whatever \n",
      "토픽- 2 : send wrong copy answer group apple lost mean must take \n",
      "토픽- 3 : window driver using program motif line work widget application mode \n",
      "토픽- 4 : article model color runs seen come three done hear tried \n",
      "토픽- 5 : drive file windows files graphics price disk would system software \n",
      "토픽- 6 : thanks please mail anyone know looking advance information help could \n",
      "토픽- 7 : card video monitor drivers info windows called hardware anyone directory \n",
      "토픽- 8 : speed sound high test love going memory works different running \n",
      "토픽- 9 : team players hockey league best water season currently matter used \n",
      "토픽-10 : would think good like might well something know really heard \n",
      "토픽-11 : anybody david part like home city read problems today cause \n",
      "토픽-12 : game games play season soon posting team first sorry second \n",
      "토픽-13 : space right subject course long original around keep years short \n",
      "토픽-14 : sale offer bike stuff sell ground call contact make cost \n",
      "토픽-15 : israel jews israeli email jewish killed anti true small talk \n",
      "토픽-16 : people would jesus think believe know like christian bible evidence \n",
      "토픽-17 : would people government think like money right state even space \n",
      "토픽-18 : chip clipper encryption keys system public software message technology phone \n",
      "토픽-19 : address university april name mark john number year washington last \n",
      "토픽-20 : would like time year back left good much know right \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(topic_term)):\n",
    "    idx = np.flipud(topic_term[i].argsort())[:10] #flipud : 배열 위 아래로 뒤집기 (순서 바꾸기)\n",
    "                                                #argsort() : 오름차 순으로 번호매김\n",
    "    print('토픽-{:2d} : '.format(i+1), end='')\n",
    "    for n in idx:\n",
    "        print('{:s} '.format(vocab[n]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flipud , argsort 활용법\n",
    "\n",
    "a=np.array([0.7,0.3,0.8])\n",
    "b=a.argsort()\n",
    "print(b) #1번 단어가 가장 영향력 적다. 0.8이 가장 가능성 있다\n",
    "np.flipud(b) #1차원이면 양옆으로만 뒤집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_dist = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(topic_word_dist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=topic_term / np sum(topic.axis=1) # axis=1 : 가로로 다 더해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 500 artists>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.bar(np.arange(0, 500), topic_word_dist[0:1,:][0]) #2개의 토픽이 걸쳐있는 모양새"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 745.86249886],\n",
       "       [ 293.81219977],\n",
       "       [ 343.10428028],\n",
       "       [ 313.96405056],\n",
       "       [ 802.99493552],\n",
       "       [ 737.16285278],\n",
       "       [1990.50135305],\n",
       "       [ 168.61621857],\n",
       "       [ 463.46760886],\n",
       "       [ 271.56628326],\n",
       "       [4264.59087949],\n",
       "       [4071.62634714],\n",
       "       [ 261.14297574],\n",
       "       [ 402.96728601],\n",
       "       [ 275.87422421],\n",
       "       [7442.00056366],\n",
       "       [7648.76144539],\n",
       "       [3585.79989254],\n",
       "       [ 513.26484123],\n",
       "       [4429.23743516]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.components_.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "x[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(10,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
