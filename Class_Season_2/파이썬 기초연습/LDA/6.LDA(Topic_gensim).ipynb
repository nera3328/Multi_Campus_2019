{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)를 이용한 뉴스 데이터 클러스터링\n",
    "gensim 패키지를 이용한다.--요약지원하는 textrank알고리즘 내장  \n",
    "--document frequency (DF) 정보가 필요하기 때문에 corpus 와 idx to word 정보가 포함된 dictionary 를 입력해야함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "#from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newsData = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./data/news.data', 'wb') as f:\n",
    "#    pickle.dump(newsData , f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/news.data', 'rb') as f:\n",
    "    newsData  = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "Well i'm not sure about the story nad it did seem biased. What\n",
      "I disagree with is your statement that the U.S. Media is out to\n",
      "ruin Israels reputation. That is rediculous. The U.S. media is\n",
      "the most pro-israeli media in the world. Having lived in Europe\n",
      "I realize that incidences such as the one described in the\n",
      "letter have occured. The U.S. media as a whole seem to try to\n",
      "ignore them. The U.S. is subsidizing Israels existance and the\n",
      "Europeans are not (at least not to the same degree). So I think\n",
      "that might be a reason they report more clearly on the\n",
      "atrocities.\n",
      "\tWhat is a shame is that in Austria, daily reports of\n",
      "the inhuman acts commited by Israeli soldiers and the blessing\n",
      "received from the Government makes some of the Holocaust guilt\n",
      "go away. After all, look how the Jews are treating other races\n",
      "when they got power. It is unfortunate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news = newsData.data\n",
    "print(len(news))\n",
    "print(news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(newsData.target_names)\n",
    "print(len(newsData.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing\n",
    "영문자가 아닌 문자를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news1 = []\n",
    "for doc in news:\n",
    "    news1.append(re.sub(\"[^a-zA-Z]\", \" \", doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어를 제거하고, 모든 단어를 소문자로 변환하고, 길이가 3 이하인 단어를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "news2 = []\n",
    "for doc in news1:\n",
    "    doc1 = []\n",
    "    for w in doc.split():\n",
    "        w = w.lower()\n",
    "        if len(w) > 3 and w not in stop_words:\n",
    "            doc1.append(w)\n",
    "    news2.append(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'sure', 'story', 'seem', 'biased', 'disagree', 'statement', 'media', 'ruin', 'israels', 'reputation', 'rediculous', 'media', 'israeli', 'media', 'world', 'lived', 'europe', 'realize', 'incidences', 'described', 'letter', 'occured', 'media', 'whole', 'seem', 'ignore', 'subsidizing', 'israels', 'existance', 'europeans', 'least', 'degree', 'think', 'might', 'reason', 'report', 'clearly', 'atrocities', 'shame', 'austria', 'daily', 'reports', 'inhuman', 'acts', 'commited', 'israeli', 'soldiers', 'blessing', 'received', 'government', 'makes', 'holocaust', 'guilt', 'away', 'look', 'jews', 'treating', 'races', 'power', 'unfortunate']\n"
     ]
    }
   ],
   "source": [
    "print(news2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doc2bow 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = corpora.Dictionary(news2) #news data diction화,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'acts',\n",
       " 1: 'atrocities',\n",
       " 2: 'austria',\n",
       " 3: 'away',\n",
       " 4: 'biased',\n",
       " 5: 'blessing',\n",
       " 6: 'clearly',\n",
       " 7: 'commited',\n",
       " 8: 'daily',\n",
       " 9: 'degree'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(dict(vocab))\n",
    "dict(list(vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64281"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_bow = [vocab.doc2bow(s) for s in news2]#document---> bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 4), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(news_bow[0])\n",
    "#(0,1 ) => voca에서 0번 단어가 1번 쓰였음( acts )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LDA(news_bow, num_topics = len(newsData.target_names), id2word=vocab)\n",
    "#           data                      20개                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic별로 문서 분류\n",
    "doc_topic 행렬에서 가장 큰 colume을 선택한다. Colume에 topic의 score가 부여돼 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = model.get_document_topics(news_bow) #문서 대 주제 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.          0.44350123]\n",
      " [ 8.          0.08121175]\n",
      " [15.          0.3186287 ]\n",
      " [16.          0.14352335]]\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "dp = np.array(doc_topic[0])\n",
    "print(dp)\n",
    "print(dp[np.argmax(dp[:, 1]), 0]) #문서 0은 topic이 7일 확률이 44%다 이런식으로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9868650287389755"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dp[:,1]) #위에 문서 0의 %를 모두(일정 확률 이상의) 더해보면 98% 거의 1이 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까진 문서 0번만 본거고 for문으로 전체로 봐보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서-1 : topic = 8\n",
      "문서-2 : topic = 16\n",
      "문서-3 : topic = 8\n",
      "문서-4 : topic = 13\n",
      "문서-5 : topic = 14\n",
      "문서-6 : topic = 15\n",
      "문서-7 : topic = 18\n",
      "문서-8 : topic = 11\n",
      "문서-9 : topic = 20\n",
      "문서-10 : topic = 18\n",
      "문서-11 : topic = 1\n",
      "문서-12 : topic = 2\n",
      "문서-13 : topic = 4\n",
      "문서-14 : topic = 13\n",
      "문서-15 : topic = 1\n",
      "문서-16 : topic = 1\n",
      "문서-17 : topic = 5\n",
      "문서-18 : topic = 8\n",
      "문서-19 : topic = 2\n",
      "문서-20 : topic = 12\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    dp = np.array(doc_topic[i])\n",
    "    most_likely_topic = int(dp[np.argmax(dp[:, 1]), 0])\n",
    "    print('문서-{:d} : topic = {:d}'.format(i+1, most_likely_topic+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### topic_term 행렬에서 topic 별로 중요 단어를 표시한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(282, 0.012816965),\n",
       " (1657, 0.010865842),\n",
       " (627, 0.0097775245),\n",
       " (2171, 0.008808282),\n",
       " (1870, 0.008058374),\n",
       " (1013, 0.007561394),\n",
       " (962, 0.007505523),\n",
       " (213, 0.0068516405),\n",
       " (2123, 0.0067374855),\n",
       " (1186, 0.005642815)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_term = model.get_topic_terms(0, topn=10)\n",
    "#                      첫번째 주제에서 / 상위 퍼센트 뽑아와\n",
    "topic_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽- 1 : file program available windows software files entry also version info \n",
      "토픽- 2 : drive information also output mail used using please ground wire \n",
      "토픽- 3 : encryption security keys public chip government clipper health number information \n",
      "토픽- 4 : bike like would good gordon soon pitt ride riding bikes \n",
      "토픽- 5 : would like guns right time people year think well could \n",
      "토픽- 6 : would government constitution water right congress amendment arms united people \n",
      "토픽- 7 : team play hockey league players player teams last myers period \n",
      "토픽- 8 : israel jews would israeli arab people right like palestinian state \n",
      "토픽- 9 : people think would know president going like something said really \n",
      "토픽-10 : freeware expos acid intake xsun points burt tranny trends would \n",
      "토픽-11 : armenian armenians people like turkish said would know turkey could \n",
      "토픽-12 : jesus bible christian christ faith christians believe would word father \n",
      "토픽-13 : would space people science also think time many research world \n",
      "토픽-14 : game games year season team first play period last chicago \n",
      "토픽-15 : entries file copies first year cover good winners stream lost \n",
      "토픽-16 : people would know think well time many even much said \n",
      "토픽-17 : sleeve picture easter insurance exhaust would surgery shoulder cure capitol \n",
      "토픽-18 : like would know system problem card anyone also need good \n",
      "토픽-19 : ripem data would public anonymous using output used time could \n",
      "토픽-20 : window scsi widget char printf null would like remark know \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(newsData.target_names)):\n",
    "    topic_term = model.get_topic_terms(i, topn=10)\n",
    "    idx = [idx for idx, score in topic_term]\n",
    "    print('토픽-{:2d} : '.format(i+1), end='')\n",
    "    for n in idx:\n",
    "        print('{:s} '.format(vocab[n]), end='')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
