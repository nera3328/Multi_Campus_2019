{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionary를 만들자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=['dog like cat', \n",
    "      'dog dog fish river',\n",
    "      'cat cute odd eye', \n",
    "      'cat dog human',\n",
    "      'dog bark cut cry',\n",
    "      'cat run dog run',\n",
    "      'cat cat cat dog dog dog',\n",
    "      'dog eat cat cat dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'like', 'cat', 'dog', 'dog', 'fish', 'river', 'cat', 'cute', 'odd', 'eye', 'cat', 'dog', 'human', 'dog', 'bark', 'cut', 'cry', 'cat', 'run', 'dog', 'run', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'dog', 'eat', 'cat', 'cat', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#단어 리스트\n",
    "lst=[]\n",
    "for i,j in enumerate(text):\n",
    "    for a in j.split():\n",
    "        lst.append(a)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat', 'eye', 'cut', 'like', 'human', 'eat', 'cute', 'river', 'bark', 'odd', 'dog', 'run', 'fish', 'cry'}\n"
     ]
    }
   ],
   "source": [
    "#lst를 set으로 집합\n",
    "vocabulary=set(lst)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionary 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 1, 'eye': 2, 'cut': 3, 'like': 4, 'human': 5, 'eat': 6, 'cute': 7, 'river': 8, 'bark': 9, 'odd': 10, 'dog': 11, 'run': 12, 'fish': 13, 'cry': 14}\n"
     ]
    }
   ],
   "source": [
    "diction={}\n",
    "for p, q in enumerate(vocabulary):\n",
    "    diction[q]= p+1\n",
    "print(diction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 수치화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 4, 1], [11, 11, 13, 8], [1, 7, 10, 2], [1, 11, 5], [11, 9, 3, 14], [1, 12, 11, 12], [1, 1, 1, 11, 11, 11], [11, 6, 1, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "#단어 리스트\n",
    "voca_lst=[]\n",
    "for txt in text:\n",
    "    dic_lst=[]\n",
    "    for word in txt.split():\n",
    "        dic_lst.append(diction[word])\n",
    "    voca_lst.append(dic_lst)\n",
    "print(voca_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제일 긴 문장에 맞추어 padding작업  \n",
    "  \n",
    "----->Encoding(텍스트를 단순히 수치화, 단어-문장 의미는 전혀 고려 X  \n",
    "-수치데이터는 단어나 문장의 의미는 전혀 반영되지 않았다)\n",
    "  \n",
    "----->word embedding : 수치데이터가 의미를 갖게 하는 것(학습 기반)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 11, 4, 1],\n",
       " [0, 0, 11, 11, 13, 8],\n",
       " [0, 0, 1, 7, 10, 2],\n",
       " [0, 0, 0, 1, 11, 5],\n",
       " [0, 0, 11, 9, 3, 14],\n",
       " [0, 0, 1, 12, 11, 12],\n",
       " [1, 1, 1, 11, 11, 11],\n",
       " [0, 11, 6, 1, 1, 11]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#제일 긴 문장 고르기-\n",
    "#1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "maxlen=np.max([len(d) for d in voca_lst])\n",
    "\n",
    "for z in range(len(voca_lst)):\n",
    "    voca_lst[z]=[0]*(maxlen -len(voca_lst[z]))+voca_lst[z]\n",
    "\n",
    "#   if maxlen < len(voca_lst[z]):\n",
    "#       maxlen=len(voca_lst[z])\n",
    "voca_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voca_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2번째 패딩법-keras\n",
    "\n",
    "voc=sequence.pad_sequences([voca_lst],maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv1D, Embedding, Input, GlobalMaxPooling1D,Flatten\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array(voca_lst)\n",
    "y_train=np.array([[0],[1],[1],[0],[0],[1],[0],[0]])\n",
    "#z_test=np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 6, 4)              60        \n",
      "=================================================================\n",
      "Total params: 60\n",
      "Trainable params: 60\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#간단 모델\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "xInput=Input(batch_shape=(None,6)) #6개 길이 문장 8개\n",
    "xEmbed=Embedding(input_dim=len(vocabulary)+1, output_dim=4, input_length=6)(xInput) #단어 1개마다 4개의 수치 vector 표현, row는 6개(문장 길이), 8개 리뷰 \n",
    "#+1안해주면 erorr, padding용을 만들어줌.              #문장의 최대 길이\n",
    "model=Model(xInput, xEmbed)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 6, 4)              60        \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 4, 12)             156       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 229\n",
      "Trainable params: 229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#진짜 모델\n",
    "K.clear_session()\n",
    "\n",
    "xInput=Input(batch_shape=(None,6))\n",
    "xEmbed=Embedding(input_dim=len(vocabulary)+1, output_dim=4, input_length=6)(xInput)\n",
    "conv=Conv1D(12,3,activation='relu')(xEmbed)\n",
    "pool=GlobalMaxPooling1D()(conv)\n",
    "xOutput=Dense(1,activation='sigmoid', batch_size=1)(pool)\n",
    "\n",
    "model=Model(xInput, xOutput)\n",
    "#model.compile(x_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.6945\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6918\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 0us/step - loss: 0.6894\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6882\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6851\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6836\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6806\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6782\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6760\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6741\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6708\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6685\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6653\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6631\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6590\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6566\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6524\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6494\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6447\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6407\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6370\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6332\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6279\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6230\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6187\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6133\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6082\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6051\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5972\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5923\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5864\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5798\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5739\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5673\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5617\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5547\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5471\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5422\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5333\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5268\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5189\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5111\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5030\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4958\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4877\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4805\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4710\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4628\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4540\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7125d30>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_text=['dog cat cat human cat cat']\n",
    "word_vec2=[]\n",
    "word_vec2.append([diction[z] for v in z_text for z in v.split()])\n",
    "testset=np.array(word_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.12060194,  0.17085634, -0.04284661, -0.18416691],\n",
       "        [ 0.02949545, -0.2660891 ,  0.07586849,  0.0193243 ],\n",
       "        [ 0.02949545, -0.2660891 ,  0.07586849,  0.0193243 ],\n",
       "        [-0.17096286, -0.1742137 , -0.13638628,  0.1818021 ],\n",
       "        [ 0.02949545, -0.2660891 ,  0.07586849,  0.0193243 ],\n",
       "        [ 0.02949545, -0.2660891 ,  0.07586849,  0.0193243 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Model(xInput, xEmbed)\n",
    "model2.predict(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_txt='dog cat cat wolf'\n",
    "                   #out of vocabulary\n",
    "\n",
    "word_vec3=[]\n",
    "\n",
    "for word in z_txt.split():\n",
    "    if word in diction:\n",
    "        word_vec3.append(diction[word])\n",
    "    else:\n",
    "        word_vec3.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "npad = maxlen - len(word_vec3)\n",
    "\n",
    "ztext=[]\n",
    "\n",
    "for i in range(npad):\n",
    "    ztext.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in word_vec3:\n",
    "    ztext.append(w)\n",
    "\n",
    "ztext = np.array(ztext).reshape(1, len(ztext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 11,  1,  1,  0]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ztext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3109434]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(ztext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 1, 1, 0]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 11,  1,  1,  0]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ztext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztext=sequence.pad_sequences([word_vec3] ,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3109434]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(ztext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 11,  1,  1,  0]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ztext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
